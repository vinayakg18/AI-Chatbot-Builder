# ðŸ¤– AI Chatbot Builder

**AI Chatbot Builder** is a full-stack, locally runnable chatbot that can understand and answer questions about uploaded PDF documents.  
Itâ€™s powered by **FastAPI**, **FAISS**, and **Ollamaâ€™s Mistral model**, with a modern **Next.js frontend** for an interactive chat experience.

This project was built to explore how local LLMs and retrieval-based architectures can create document-aware AI assistants without relying on external APIs or cloud costs.

---

## ðŸš€ Key Features

- ðŸ“„ Chat with your documents â€” upload any PDF and ask natural questions about its contents  
- ðŸ§  Local AI inference using **Mistral** through **Ollama**, keeping everything on your machine  
- âš¡ Fast retrieval powered by **FAISS** for accurate context search  
- ðŸŒ Clean dark-themed **Next.js + TailwindCSS** frontend  
- ðŸ³ Fully containerized with **Docker Compose**  
- ðŸ”’ 100% privacy â€” no external API calls, no data leaves your system  

---

## ðŸ§© Architecture Overview

```
PDF â†’ Text Extraction â†’ FAISS Indexing â†’ Query Embedding
        â†“                         â†‘
     User Query â”€â”€â”€â”€â”€â–º FastAPI Backend â”€â”€â”€â”€â”€â–º Ollama (Mistral)
                                       â†“
                                AI Response
                                       â†“
                                Next.js Frontend
```

**Tech Stack**
- **Frontend:** Next.js 15, React, TailwindCSS  
- **Backend:** FastAPI, FAISS, Sentence Transformers, PyPDF2  
- **AI Model:** Mistral via Ollama  
- **Containerization:** Docker + Docker Compose  

---

## ðŸ§  How It Works

1. Upload a PDF or document.  
2. The backend extracts text, splits it into chunks, and creates embeddings using Sentence Transformers.  
3. Chunks are stored in a FAISS index for fast semantic retrieval.  
4. When you ask a question, the backend finds the most relevant chunks and builds a context prompt.  
5. The context and question are passed to **Mistral** through **Ollama**, which generates a natural-language answer.  

---

## âš™ï¸ Local Setup

### Prerequisites
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed  
- [Ollama](https://ollama.ai) installed with Mistral model pulled  

```bash
ollama pull mistral
ollama serve
```

---

### 1ï¸âƒ£ Clone this repository
```bash
git clone https://github.com/vinayakg18/AI-Chatbot-Builder.git
cd AI-Chatbot-Builder
```

### 2ï¸âƒ£ Start backend + frontend
```bash
docker-compose up --build
```

- Backend â†’ http://localhost:8000  
- Frontend â†’ http://localhost:3000  

### 3ï¸âƒ£ Verify Ollama
```bash
curl http://localhost:11434/api/tags
```
Expected output lists `mistral:latest`

---

## ðŸ§ª Usage Demo

1. Go to http://localhost:3000  
2. Upload any PDF (contract, article, etc.)  
3. Ask questions like  
   - â€œWhat is the salary mentioned?â€  
   - â€œWho is the employer?â€  
   - â€œWhat is the employeeâ€™s address?â€  
4. The chatbot will read from your uploaded file and answer contextually.

---

## ðŸ³ Docker Architecture

```yaml
services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend/storage:/app/storage
    environment:
      - PORT=8000
    extra_hosts:
      - "host.docker.internal:host-gateway"

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
```

This ensures the backend connects to **Ollama on your host machine**, and all vector data persists under `backend/storage`.

---

## ðŸ§° Folder Structure

```
AI-Chatbot-Builder/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ storage/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/app/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ðŸ“¦ Dependencies

**Backend**
- fastapi  
- uvicorn  
- faiss-cpu  
- sentence-transformers  
- PyPDF2  
- docx2txt  
- pandas  
- numpy  
- requests  

**Frontend**
- next  
- react  
- tailwindcss  

---

## ðŸ§  Example Outputs: Based on an offer letter

| Query | Response |
|-------|-----------|
| â€œWhat is my salary?â€ | â€œYour salary is $80.00 USD per hour, and $70.00 USD during training.â€ |
| â€œWho is the employer?â€ | â€œXYZ, located at ### St, San Francisco, CA 94107.â€ |
| â€œWho is the employee?â€ | â€œVinayakraddi Giriyammanavar.â€ |

---

## ðŸ› ï¸ Future Enhancements

- Support for multiple PDFs  
- Optional Hugging Face / Groq API fallback  
- User authentication  
- Persistent conversation memory  
- Deployment via Render, Fly.io, Railway, or VPS  

---

## ðŸ‘¨â€ðŸ’» Author

**Vinayakraddi Giriyammanavar**  
Graduate Student | Software Engineer | AI Enthusiast  

- GitHub â†’ [vinayakg18](https://github.com/vinayakg18)  
- LinkedIn â†’ [Vinayakraddi Giriyammanavar](https://linkedin.com/in/vinayakraddi-giriyammanavar)

---

## ðŸ§¾ License

This project is released under the **MIT License**.  
Feel free to fork, modify, and experiment â€” attribution is appreciated!

---

> _Built for learning and exploration. This project shows how modern LLMs like Mistral can run locally to enable private, document-aware AI assistants._
